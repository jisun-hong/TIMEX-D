{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Word2vec_MFCC_Minmax_Multiply"
      ],
      "metadata": {
        "id": "uJcVR9ifZCuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D, Dropout, LayerNormalization, Multiply\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from gensim.models import KeyedVectors  # Word2Vec 모델을 사용하기 위해 추가\n",
        "\n",
        "# Load labels\n",
        "labels_path = \"D:/EDAIC-WOZ/labels/normal_label2.xlsx\"\n",
        "labels_df = pd.read_excel(labels_path)\n",
        "labels_df = labels_df[['Participant_ID', 'PHQ_Three']]\n",
        "\n",
        "# Directories for text and audio data\n",
        "source_root_text = \"D:/EDAIC-WOZ/Processed_transcript/three_level2\"\n",
        "source_root_audio = \"D:/EDAIC-WOZ/audio_token_level/three_level2\"\n",
        "\n",
        "# Initialize lists to hold data and labels\n",
        "all_sentences = []\n",
        "all_mfcc_features = []\n",
        "label_list = []\n",
        "\n",
        "# Function to load and preprocess individual CSV files\n",
        "def load_and_preprocess_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return ' '.join(df['Text'].tolist())  # Combine all sentences into one\n",
        "def extract_aggregate_mfcc_minmax(audio_files, max_pad_len=100, n_mfcc=128):\n",
        "    mfcc_features_list = []\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    for audio_path in audio_files:\n",
        "        if os.path.exists(audio_path):\n",
        "            audio, sample_rate = librosa.load(audio_path, sr=None)\n",
        "            mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "\n",
        "            if mfcc.shape[1] < max_pad_len:\n",
        "                pad_width = max_pad_len - mfcc.shape[1]\n",
        "                mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "            else:\n",
        "                mfcc = mfcc[:, :max_pad_len]\n",
        "\n",
        "            # Apply MinMax Scaling\n",
        "            mfcc_scaled = scaler.fit_transform(mfcc.T).T\n",
        "            mfcc_features_list.append(mfcc_scaled.T)\n",
        "\n",
        "    if len(mfcc_features_list) > 0:\n",
        "        # Aggregate MFCC features (e.g., by taking the mean)\n",
        "        aggregated_mfcc = np.mean(mfcc_features_list, axis=0)\n",
        "    else:\n",
        "        aggregated_mfcc = np.zeros((max_pad_len, n_mfcc))  # Fallback if no audio found\n",
        "\n",
        "    return aggregated_mfcc\n",
        "\n",
        "# Replace the MFCC extraction function with the MinMaxScaling version\n",
        "for subfolder in os.listdir(source_root_audio):\n",
        "    participant_id, subfolder_num = subfolder.split('_')\n",
        "\n",
        "    # Determine the corresponding CSV file for this subfolder\n",
        "    csv_file = f'{participant_id}_{subfolder_num}_processed.csv'\n",
        "    text_file_path = os.path.join(source_root_text, csv_file)\n",
        "\n",
        "    if not os.path.exists(text_file_path):\n",
        "        print(f'Text file {text_file_path} not found for audio folder {subfolder}. Skipping.')\n",
        "        continue\n",
        "\n",
        "    # Load and aggregate sentences from the CSV\n",
        "    aggregated_sentence = load_and_preprocess_csv(text_file_path)\n",
        "\n",
        "    label_row = labels_df[labels_df['Participant_ID'] == int(participant_id)]\n",
        "\n",
        "    if label_row.empty:\n",
        "        print(f\"Label not found for Participant ID: {participant_id}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    phq_three = label_row['PHQ_Three'].values[0]\n",
        "\n",
        "    subfolder_path = os.path.join(source_root_audio, subfolder)\n",
        "    audio_files = [os.path.join(subfolder_path, f'{participant_id}{subfolder_num}_{i}.wav') for i in range(len(aggregated_sentence.split()))]\n",
        "\n",
        "    # Aggregate MFCC features from the audio files using MinMaxScaling\n",
        "    aggregated_mfcc = extract_aggregate_mfcc_minmax(audio_files)\n",
        "\n",
        "    # Append the aggregated sentence and MFCC to the lists\n",
        "    all_sentences.append(aggregated_sentence)\n",
        "    all_mfcc_features.append(aggregated_mfcc)\n",
        "    label_list.append(phq_three)\n"
      ],
      "metadata": {
        "id": "B8yUKBrqZDi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Tokenize and prepare Word2Vec embedding\n",
        "# Tokenize the sentences\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D, Dropout, LayerNormalization, Multiply\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Load pretrained Word2Vec model\n",
        "word2vec_model = Word2Vec.load(\"D:/EDAIC-WOZ/word2vec_sgns_custom.model\")\n",
        "\n",
        "# Create embedding matrix using the pretrained Word2Vec model\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "vocab_size = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "# Fill the embedding matrix with pretrained word vectors (gensim 4.x compatibility)\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv.key_to_index:  # Use key_to_index in gensim 4.x\n",
        "        embedding_vector = word2vec_model.wv[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define Word2Vec Embedding Layer\n",
        "embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=100,  # max_length\n",
        "                            trainable=True)  # Don't train the embedding weights\n",
        "\n",
        "# Convert the sentences to sequences\n",
        "sequences = tokenizer.texts_to_sequences(all_sentences)\n",
        "\n",
        "# Pad the sequences to ensure uniform length\n",
        "max_length = 100  # Adjust based on your data\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# Convert MFCC features to numpy array\n",
        "mfcc_features = np.array(all_mfcc_features)\n",
        "\n",
        "# Convert labels to categorical format\n",
        "categorical_labels = to_categorical(label_list, num_classes=3)\n",
        "\n",
        "# Define Transformer block\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerBlock, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"rate\": self.rate,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "# Define model using Word2Vec embedding\n",
        "def create_transformer_model(text_input_shape, mfcc_input_shape):\n",
        "    text_inputs = Input(shape=text_input_shape)\n",
        "    embedded_sequences = embedding_layer(text_inputs)\n",
        "\n",
        "    mfcc_inputs = Input(shape=mfcc_input_shape)\n",
        "    mfcc_dense = Dense(128, activation=\"relu\")(mfcc_inputs)\n",
        "\n",
        "    combined = Multiply()([embedded_sequences, mfcc_dense])\n",
        "\n",
        "    transformer_block = TransformerBlock(embed_dim=128, num_heads=4, ff_dim=128)\n",
        "    x = transformer_block(combined)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    outputs = Dense(3, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=[text_inputs, mfcc_inputs], outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# KFold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=119)\n",
        "fold_results = []\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(padded_sequences)):\n",
        "    print(f\"Training fold {fold + 1}/5...\")\n",
        "\n",
        "    X_text_train, X_text_test = padded_sequences[train_index], padded_sequences[test_index]\n",
        "    X_mfcc_train, X_mfcc_test = mfcc_features[train_index], mfcc_features[test_index]\n",
        "    y_train, y_test = categorical_labels[train_index], categorical_labels[test_index]\n",
        "\n",
        "    X_text_train, X_text_val, X_mfcc_train, X_mfcc_val, y_train, y_val = train_test_split(\n",
        "        X_text_train, X_mfcc_train, y_train, test_size=0.2, random_state=119, shuffle=True\n",
        "    )\n",
        "\n",
        "    text_input_shape = (max_length,)\n",
        "    mfcc_input_shape = (100, 128)\n",
        "\n",
        "    model = create_transformer_model(text_input_shape, mfcc_input_shape)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    model_checkpoint = ModelCheckpoint(f'depression_diagnosis_model_fold_{fold+1}.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "    history = model.fit([X_text_train, X_mfcc_train], y_train, epochs=50, batch_size=16,\n",
        "                        validation_data=([X_text_val, X_mfcc_val], y_val),\n",
        "                        callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "    model = tf.keras.models.load_model(f'depression_diagnosis_model_fold_{fold+1}.keras', custom_objects={'TransformerBlock': TransformerBlock})\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    loss, accuracy = model.evaluate([X_text_test, X_mfcc_test], y_test)\n",
        "    print(f'Fold {fold + 1} Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Collect the fold results\n",
        "    fold_results.append((loss, accuracy))\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = model\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict([X_text_test, X_mfcc_test])\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Compute F1 score, precision, recall\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    accuracy = np.mean(y_true_classes == y_pred_classes)\n",
        "\n",
        "    print(f'Fold {fold + 1} F1 Score: {f1:.2f}')\n",
        "    print(f'Fold {fold + 1} Precision: {precision:.2f}')\n",
        "    print(f'Fold {fold + 1} Recall: {recall:.2f}')\n",
        "    print(f'Fold {fold + 1} Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Confusion Matrix for the current fold\n",
        "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "    # Normalize the confusion matrix by row (true classes)\n",
        "    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
        "                xticklabels=['Non-depressed', 'Mildly depressed', 'Severely depressed'],\n",
        "                yticklabels=['Non-depressed', 'Mildly depressed', 'Severely depressed'],\n",
        "                annot_kws={'size': 16})  # Font size for annotations\n",
        "    plt.xlabel('Predicted', fontsize=14)\n",
        "    plt.ylabel('True', fontsize=14)\n",
        "    plt.title(f'Confusion Matrix - Fold {fold + 1}', fontsize=16)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation accuracy and loss values\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'Fold {fold + 1} - Model Accuracy', fontsize=16)\n",
        "    plt.ylabel('Accuracy', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'Fold {fold + 1} - Model Loss', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Calculate and print average loss and accuracy across all folds\n",
        "average_loss = np.mean([result[0] for result in fold_results])\n",
        "average_accuracy = np.mean([result[1] for result in fold_results])\n",
        "print(f\"Average Test Accuracy across 5 folds: {average_accuracy * 100:.2f}%\")\n",
        "print(f\"Average Test Loss across 5 folds: {average_loss:.4f}\")\n",
        "\n",
        "# 모델 저장\n",
        "if best_model is not None:\n",
        "    best_model.save('D:/EDAIC-WOZ/best_model/three_level2/mfcc_minmax_multiply_word2vec.h5')\n",
        "    print(f\"Best model saved as mfcc_minmax_multiply_word2vec.h5\")"
      ],
      "metadata": {
        "id": "E7YixV6fZCVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec_MFCC_Standard_Multiply"
      ],
      "metadata": {
        "id": "tJV-HZyxZ4zB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqSD8C57Y0-i"
      },
      "outputs": [],
      "source": [
        "#%% MFCC Standard scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Function to extract and aggregate MFCC features from audio files with StandardScaling\n",
        "def extract_aggregate_mfcc_standard(audio_files, max_pad_len=100, n_mfcc=128):\n",
        "    mfcc_features_list = []\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    for audio_path in audio_files:\n",
        "        if os.path.exists(audio_path):\n",
        "            audio, sample_rate = librosa.load(audio_path, sr=None)\n",
        "            mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "\n",
        "            if mfcc.shape[1] < max_pad_len:\n",
        "                pad_width = max_pad_len - mfcc.shape[1]\n",
        "                mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "            else:\n",
        "                mfcc = mfcc[:, :max_pad_len]\n",
        "\n",
        "            # Apply Standard Scaling\n",
        "            mfcc_scaled = scaler.fit_transform(mfcc.T).T\n",
        "            mfcc_features_list.append(mfcc_scaled.T)\n",
        "\n",
        "    if len(mfcc_features_list) > 0:\n",
        "        # Aggregate MFCC features (e.g., by taking the mean)\n",
        "        aggregated_mfcc = np.mean(mfcc_features_list, axis=0)\n",
        "    else:\n",
        "        aggregated_mfcc = np.zeros((max_pad_len, n_mfcc))  # Fallback if no audio found\n",
        "\n",
        "    return aggregated_mfcc\n",
        "\n",
        "# Replace the MFCC extraction function with the StandardScaling version\n",
        "for subfolder in os.listdir(source_root_audio):\n",
        "    participant_id, subfolder_num = subfolder.split('_')\n",
        "\n",
        "    # Determine the corresponding CSV file for this subfolder\n",
        "    csv_file = f'{participant_id}_{subfolder_num}_processed.csv'\n",
        "    text_file_path = os.path.join(source_root_text, csv_file)\n",
        "\n",
        "    if not os.path.exists(text_file_path):\n",
        "        print(f'Text file {text_file_path} not found for audio folder {subfolder}. Skipping.')\n",
        "        continue\n",
        "\n",
        "    # Load and aggregate sentences from the CSV\n",
        "    aggregated_sentence = load_and_preprocess_csv(text_file_path)\n",
        "\n",
        "    label_row = labels_df[labels_df['Participant_ID'] == int(participant_id)]\n",
        "\n",
        "    if label_row.empty:\n",
        "        print(f\"Label not found for Participant ID: {participant_id}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    phq_binary = label_row['PHQ_Three'].values[0]\n",
        "\n",
        "    subfolder_path = os.path.join(source_root_audio, subfolder)\n",
        "    audio_files = [os.path.join(subfolder_path, f'{participant_id}{subfolder_num}_{i}.wav') for i in range(len(aggregated_sentence.split()))]\n",
        "\n",
        "    # Aggregate MFCC features from the audio files using StandardScaling\n",
        "    aggregated_mfcc = extract_aggregate_mfcc_standard(audio_files)\n",
        "\n",
        "    # Append the aggregated sentence and MFCC to the lists\n",
        "    all_sentences.append(aggregated_sentence)\n",
        "    all_mfcc_features.append(aggregated_mfcc)\n",
        "    label_list.append(phq_binary)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Tokenize and prepare Word2Vec embedding\n",
        "# Tokenize the sentences\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D, Dropout, LayerNormalization, Multiply\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Load pretrained Word2Vec model\n",
        "word2vec_model = Word2Vec.load(\"D:/EDAIC-WOZ/word2vec_sgns_custom.model\")\n",
        "\n",
        "# Create embedding matrix using the pretrained Word2Vec model\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "vocab_size = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "# Fill the embedding matrix with pretrained word vectors (gensim 4.x compatibility)\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv.key_to_index:  # Use key_to_index in gensim 4.x\n",
        "        embedding_vector = word2vec_model.wv[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define Word2Vec Embedding Layer\n",
        "embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=100,  # max_length\n",
        "                            trainable=True)  # Don't train the embedding weights\n",
        "\n",
        "# Convert the sentences to sequences\n",
        "sequences = tokenizer.texts_to_sequences(all_sentences)\n",
        "\n",
        "# Pad the sequences to ensure uniform length\n",
        "max_length = 100  # Adjust based on your data\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# Convert MFCC features to numpy array\n",
        "mfcc_features = np.array(all_mfcc_features)\n",
        "\n",
        "# Convert labels to categorical format\n",
        "categorical_labels = to_categorical(label_list, num_classes=3)\n",
        "\n",
        "# Define Transformer block\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerBlock, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"rate\": self.rate,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "# Define model using Word2Vec embedding\n",
        "def create_transformer_model(text_input_shape, mfcc_input_shape):\n",
        "    text_inputs = Input(shape=text_input_shape)\n",
        "    embedded_sequences = embedding_layer(text_inputs)\n",
        "\n",
        "    mfcc_inputs = Input(shape=mfcc_input_shape)\n",
        "    mfcc_dense = Dense(128, activation=\"relu\")(mfcc_inputs)\n",
        "\n",
        "    combined = Multiply()([embedded_sequences, mfcc_dense])\n",
        "\n",
        "    transformer_block = TransformerBlock(embed_dim=128, num_heads=4, ff_dim=128)\n",
        "    x = transformer_block(combined)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    outputs = Dense(3, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=[text_inputs, mfcc_inputs], outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# KFold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=119)\n",
        "fold_results = []\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(padded_sequences)):\n",
        "    print(f\"Training fold {fold + 1}/5...\")\n",
        "\n",
        "    X_text_train, X_text_test = padded_sequences[train_index], padded_sequences[test_index]\n",
        "    X_mfcc_train, X_mfcc_test = mfcc_features[train_index], mfcc_features[test_index]\n",
        "    y_train, y_test = categorical_labels[train_index], categorical_labels[test_index]\n",
        "\n",
        "    X_text_train, X_text_val, X_mfcc_train, X_mfcc_val, y_train, y_val = train_test_split(\n",
        "        X_text_train, X_mfcc_train, y_train, test_size=0.2, random_state=119, shuffle=True\n",
        "    )\n",
        "\n",
        "    text_input_shape = (max_length,)\n",
        "    mfcc_input_shape = (100, 128)\n",
        "\n",
        "    model = create_transformer_model(text_input_shape, mfcc_input_shape)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    model_checkpoint = ModelCheckpoint(f'depression_diagnosis_model_fold_{fold+1}.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "    history = model.fit([X_text_train, X_mfcc_train], y_train, epochs=50, batch_size=16,\n",
        "                        validation_data=([X_text_val, X_mfcc_val], y_val),\n",
        "                        callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "    model = tf.keras.models.load_model(f'depression_diagnosis_model_fold_{fold+1}.keras', custom_objects={'TransformerBlock': TransformerBlock})\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    loss, accuracy = model.evaluate([X_text_test, X_mfcc_test], y_test)\n",
        "    print(f'Fold {fold + 1} Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Collect the fold results\n",
        "    fold_results.append((loss, accuracy))\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = model\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict([X_text_test, X_mfcc_test])\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Compute F1 score, precision, recall\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    accuracy = np.mean(y_true_classes == y_pred_classes)\n",
        "\n",
        "    print(f'Fold {fold + 1} F1 Score: {f1:.2f}')\n",
        "    print(f'Fold {fold + 1} Precision: {precision:.2f}')\n",
        "    print(f'Fold {fold + 1} Recall: {recall:.2f}')\n",
        "    print(f'Fold {fold + 1} Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Confusion Matrix for the current fold\n",
        "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "    # Normalize the confusion matrix by row (true classes)\n",
        "    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
        "                xticklabels=['Non-depressed', 'Mildly depressed', 'Severely depressed'],\n",
        "                yticklabels=['Non-depressed', 'Mildly depressed', 'Severely depressed'],\n",
        "                annot_kws={'size': 16})  # Font size for annotations\n",
        "    plt.xlabel('Predicted', fontsize=14)\n",
        "    plt.ylabel('True', fontsize=14)\n",
        "    plt.title(f'Confusion Matrix - Fold {fold + 1}', fontsize=16)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation accuracy and loss values\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'Fold {fold + 1} - Model Accuracy', fontsize=16)\n",
        "    plt.ylabel('Accuracy', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'Fold {fold + 1} - Model Loss', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Calculate and print average loss and accuracy across all folds\n",
        "average_loss = np.mean([result[0] for result in fold_results])\n",
        "average_accuracy = np.mean([result[1] for result in fold_results])\n",
        "print(f\"Average Test Accuracy across 5 folds: {average_accuracy * 100:.2f}%\")\n",
        "print(f\"Average Test Loss across 5 folds: {average_loss:.4f}\")\n",
        "\n",
        "# 모델 저장\n",
        "if best_model is not None:\n",
        "    best_model.save('D:/EDAIC-WOZ/best_model/three_level2/mfcc_standard_multiply_word2vec.h5')\n",
        "    print(f\"Best model saved as mfcc_minmax_multiply_word2vec.h5\")"
      ],
      "metadata": {
        "id": "Ean5DB7jZ2Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec_MFCC_Minmax_Stadnard_Multiply"
      ],
      "metadata": {
        "id": "-NNrpsmKaCRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% MFCC Minmaxscaling\n",
        "# Function to extract and aggregate MFCC features from audio files with MinMaxScaling\n",
        "def extract_aggregate_mfcc_minmax(audio_files, max_pad_len=100, n_mfcc=128):\n",
        "    mfcc_features_list = []\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    for audio_path in audio_files:\n",
        "        if os.path.exists(audio_path):\n",
        "            audio, sample_rate = librosa.load(audio_path, sr=None)\n",
        "            mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "\n",
        "            if mfcc.shape[1] < max_pad_len:\n",
        "                pad_width = max_pad_len - mfcc.shape[1]\n",
        "                mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "            else:\n",
        "                mfcc = mfcc[:, :max_pad_len]\n",
        "\n",
        "            # Apply MinMax Scaling\n",
        "            mfcc_scaled = scaler.fit_transform(mfcc.T).T\n",
        "            mfcc_features_list.append(mfcc_scaled.T)\n",
        "\n",
        "    if len(mfcc_features_list) > 0:\n",
        "        # Aggregate MFCC features (e.g., by taking the mean)\n",
        "        aggregated_mfcc = np.mean(mfcc_features_list, axis=0)\n",
        "    else:\n",
        "        aggregated_mfcc = np.zeros((max_pad_len, n_mfcc))  # Fallback if no audio found\n",
        "\n",
        "    return aggregated_mfcc\n",
        "\n",
        "# Replace the MFCC extraction function with the MinMaxScaling version\n",
        "for subfolder in os.listdir(source_root_audio):\n",
        "    participant_id, subfolder_num = subfolder.split('_')\n",
        "\n",
        "    # Determine the corresponding CSV file for this subfolder\n",
        "    csv_file = f'{participant_id}_{subfolder_num}_processed.csv'\n",
        "    text_file_path = os.path.join(source_root_text, csv_file)\n",
        "\n",
        "    if not os.path.exists(text_file_path):\n",
        "        print(f'Text file {text_file_path} not found for audio folder {subfolder}. Skipping.')\n",
        "        continue\n",
        "\n",
        "    # Load and aggregate sentences from the CSV\n",
        "    aggregated_sentence = load_and_preprocess_csv(text_file_path)\n",
        "\n",
        "    label_row = labels_df[labels_df['Participant_ID'] == int(participant_id)]\n",
        "\n",
        "    if label_row.empty:\n",
        "        print(f\"Label not found for Participant ID: {participant_id}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    phq_three = label_row['PHQ_Three'].values[0]\n",
        "\n",
        "    subfolder_path = os.path.join(source_root_audio, subfolder)\n",
        "    audio_files = [os.path.join(subfolder_path, f'{participant_id}{subfolder_num}_{i}.wav') for i in range(len(aggregated_sentence.split()))]\n",
        "\n",
        "    # Aggregate MFCC features from the audio files using MinMaxScaling\n",
        "    aggregated_mfcc = extract_aggregate_mfcc_minmax(audio_files)\n",
        "\n",
        "    # Append the aggregated sentence and MFCC to the lists\n",
        "    all_sentences.append(aggregated_sentence)\n",
        "    all_mfcc_features.append(aggregated_mfcc)\n",
        "    label_list.append(phq_three)\n",
        "\n",
        "#%% MFCC StandardScaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Function to extract and aggregate MFCC features from audio files with StandardScaling\n",
        "def extract_aggregate_mfcc_standard(audio_files, max_pad_len=100, n_mfcc=128):\n",
        "    mfcc_features_list = []\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    for audio_path in audio_files:\n",
        "        if os.path.exists(audio_path):\n",
        "            audio, sample_rate = librosa.load(audio_path, sr=None)\n",
        "            mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "\n",
        "            if mfcc.shape[1] < max_pad_len:\n",
        "                pad_width = max_pad_len - mfcc.shape[1]\n",
        "                mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "            else:\n",
        "                mfcc = mfcc[:, :max_pad_len]\n",
        "\n",
        "            # Apply Standard Scaling\n",
        "            mfcc_scaled = scaler.fit_transform(mfcc.T).T\n",
        "            mfcc_features_list.append(mfcc_scaled.T)\n",
        "\n",
        "    if len(mfcc_features_list) > 0:\n",
        "        # Aggregate MFCC features (e.g., by taking the mean)\n",
        "        aggregated_mfcc = np.mean(mfcc_features_list, axis=0)\n",
        "    else:\n",
        "        aggregated_mfcc = np.zeros((max_pad_len, n_mfcc))  # Fallback if no audio found\n",
        "\n",
        "    return aggregated_mfcc\n",
        "\n",
        "# Replace the MFCC extraction function with the StandardScaling version\n",
        "for subfolder in os.listdir(source_root_audio):\n",
        "    participant_id, subfolder_num = subfolder.split('_')\n",
        "\n",
        "    # Determine the corresponding CSV file for this subfolder\n",
        "    csv_file = f'{participant_id}_{subfolder_num}_processed.csv'\n",
        "    text_file_path = os.path.join(source_root_text, csv_file)\n",
        "\n",
        "    if not os.path.exists(text_file_path):\n",
        "        print(f'Text file {text_file_path} not found for audio folder {subfolder}. Skipping.')\n",
        "        continue\n",
        "\n",
        "    # Load and aggregate sentences from the CSV\n",
        "    aggregated_sentence = load_and_preprocess_csv(text_file_path)\n",
        "\n",
        "    label_row = labels_df[labels_df['Participant_ID'] == int(participant_id)]\n",
        "\n",
        "    if label_row.empty:\n",
        "        print(f\"Label not found for Participant ID: {participant_id}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    phq_three = label_row['PHQ_Three'].values[0]\n",
        "\n",
        "    subfolder_path = os.path.join(source_root_audio, subfolder)\n",
        "    audio_files = [os.path.join(subfolder_path, f'{participant_id}{subfolder_num}_{i}.wav') for i in range(len(aggregated_sentence.split()))]\n",
        "\n",
        "    # Aggregate MFCC features from the audio files using StandardScaling\n",
        "    aggregated_mfcc = extract_aggregate_mfcc_standard(audio_files)\n",
        "\n",
        "    # Append the aggregated sentence and MFCC to the lists\n",
        "    all_sentences.append(aggregated_sentence)\n",
        "    all_mfcc_features.append(aggregated_mfcc)\n",
        "    label_list.append(phq_three)\n"
      ],
      "metadata": {
        "id": "IGbNOQwUaN1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Tokenize and prepare Word2Vec embedding\n",
        "# Tokenize the sentences\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D, Dropout, LayerNormalization, Multiply\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Load pretrained Word2Vec model\n",
        "word2vec_model = Word2Vec.load(\"D:/EDAIC-WOZ/word2vec_sgns_custom.model\")\n",
        "\n",
        "# Create embedding matrix using the pretrained Word2Vec model\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "vocab_size = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "# Fill the embedding matrix with pretrained word vectors (gensim 4.x compatibility)\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv.key_to_index:  # Use key_to_index in gensim 4.x\n",
        "        embedding_vector = word2vec_model.wv[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define Word2Vec Embedding Layer\n",
        "embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=100,  # max_length\n",
        "                            trainable=True)  # Don't train the embedding weights\n",
        "\n",
        "# Convert the sentences to sequences\n",
        "sequences = tokenizer.texts_to_sequences(all_sentences)\n",
        "\n",
        "# Pad the sequences to ensure uniform length\n",
        "max_length = 100  # Adjust based on your data\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# Convert MFCC features to numpy array\n",
        "mfcc_features = np.array(all_mfcc_features)\n",
        "\n",
        "# Convert labels to categorical format\n",
        "categorical_labels = to_categorical(label_list, num_classes=3)\n",
        "\n",
        "# Define Transformer block\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerBlock, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"rate\": self.rate,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "# Define model using Word2Vec embedding\n",
        "def create_transformer_model(text_input_shape, mfcc_input_shape):\n",
        "    text_inputs = Input(shape=text_input_shape)\n",
        "    embedded_sequences = embedding_layer(text_inputs)\n",
        "\n",
        "    mfcc_inputs = Input(shape=mfcc_input_shape)\n",
        "    mfcc_dense = Dense(128, activation=\"relu\")(mfcc_inputs)\n",
        "\n",
        "    combined = Multiply()([embedded_sequences, mfcc_dense])\n",
        "\n",
        "    transformer_block = TransformerBlock(embed_dim=128, num_heads=4, ff_dim=128)\n",
        "    x = transformer_block(combined)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    outputs = Dense(3, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=[text_inputs, mfcc_inputs], outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# KFold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=119)\n",
        "fold_results = []\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(padded_sequences)):\n",
        "    print(f\"Training fold {fold + 1}/5...\")\n",
        "\n",
        "    X_text_train, X_text_test = padded_sequences[train_index], padded_sequences[test_index]\n",
        "    X_mfcc_train, X_mfcc_test = mfcc_features[train_index], mfcc_features[test_index]\n",
        "    y_train, y_test = categorical_labels[train_index], categorical_labels[test_index]\n",
        "\n",
        "    X_text_train, X_text_val, X_mfcc_train, X_mfcc_val, y_train, y_val = train_test_split(\n",
        "        X_text_train, X_mfcc_train, y_train, test_size=0.2, random_state=119, shuffle=True\n",
        "    )\n",
        "\n",
        "    text_input_shape = (max_length,)\n",
        "    mfcc_input_shape = (100, 128)\n",
        "\n",
        "    model = create_transformer_model(text_input_shape, mfcc_input_shape)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    model_checkpoint = ModelCheckpoint(f'depression_diagnosis_model_fold_{fold+1}.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "    history = model.fit([X_text_train, X_mfcc_train], y_train, epochs=50, batch_size=16,\n",
        "                        validation_data=([X_text_val, X_mfcc_val], y_val),\n",
        "                        callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "    model = tf.keras.models.load_model(f'depression_diagnosis_model_fold_{fold+1}.keras', custom_objects={'TransformerBlock': TransformerBlock})\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    loss, accuracy = model.evaluate([X_text_test, X_mfcc_test], y_test)\n",
        "    print(f'Fold {fold + 1} Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Collect the fold results\n",
        "    fold_results.append((loss, accuracy))\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = model\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict([X_text_test, X_mfcc_test])\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Compute F1 score, precision, recall\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    accuracy = np.mean(y_true_classes == y_pred_classes)\n",
        "\n",
        "    print(f'Fold {fold + 1} F1 Score: {f1:.2f}')\n",
        "    print(f'Fold {fold + 1} Precision: {precision:.2f}')\n",
        "    print(f'Fold {fold + 1} Recall: {recall:.2f}')\n",
        "    print(f'Fold {fold + 1} Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Confusion Matrix for the current fold\n",
        "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "    # Normalize the confusion matrix by row (true classes)\n",
        "    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
        "                xticklabels=['Non-depressed', 'Mildly depressed', 'Severely depressed'],\n",
        "                yticklabels=['Non-depressed', 'Mildly depressed', 'Severely depressed'],\n",
        "                annot_kws={'size': 16})  # Font size for annotations\n",
        "    plt.xlabel('Predicted', fontsize=14)\n",
        "    plt.ylabel('True', fontsize=14)\n",
        "    plt.title(f'Confusion Matrix - Fold {fold + 1}', fontsize=16)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation accuracy and loss values\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'Fold {fold + 1} - Model Accuracy', fontsize=16)\n",
        "    plt.ylabel('Accuracy', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'Fold {fold + 1} - Model Loss', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Calculate and print average loss and accuracy across all folds\n",
        "average_loss = np.mean([result[0] for result in fold_results])\n",
        "average_accuracy = np.mean([result[1] for result in fold_results])\n",
        "print(f\"Average Test Accuracy across 5 folds: {average_accuracy * 100:.2f}%\")\n",
        "print(f\"Average Test Loss across 5 folds: {average_loss:.4f}\")\n",
        "\n",
        "# 모델 저장\n",
        "if best_model is not None:\n",
        "    best_model.save('D:/EDAIC-WOZ/best_model/three_level2/mfcc_minmax_multiply_word2vec.h5')\n",
        "    print(f\"Best model saved as mfcc_minmax_multiply_word2vec.h5\")"
      ],
      "metadata": {
        "id": "klbVE9w3Z9kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2vec_MFCC_Standard_Minmax_Multiply"
      ],
      "metadata": {
        "id": "7ZRHYfPBZfhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% MFCC StandardScaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Function to extract and aggregate MFCC features from audio files with StandardScaling\n",
        "def extract_aggregate_mfcc_standard(audio_files, max_pad_len=100, n_mfcc=128):\n",
        "    mfcc_features_list = []\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    for audio_path in audio_files:\n",
        "        if os.path.exists(audio_path):\n",
        "            audio, sample_rate = librosa.load(audio_path, sr=None)\n",
        "            mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "\n",
        "            if mfcc.shape[1] < max_pad_len:\n",
        "                pad_width = max_pad_len - mfcc.shape[1]\n",
        "                mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "            else:\n",
        "                mfcc = mfcc[:, :max_pad_len]\n",
        "\n",
        "            # Apply Standard Scaling\n",
        "            mfcc_scaled = scaler.fit_transform(mfcc.T).T\n",
        "            mfcc_features_list.append(mfcc_scaled.T)\n",
        "\n",
        "    if len(mfcc_features_list) > 0:\n",
        "        # Aggregate MFCC features (e.g., by taking the mean)\n",
        "        aggregated_mfcc = np.mean(mfcc_features_list, axis=0)\n",
        "    else:\n",
        "        aggregated_mfcc = np.zeros((max_pad_len, n_mfcc))  # Fallback if no audio found\n",
        "\n",
        "    return aggregated_mfcc\n",
        "\n",
        "# Replace the MFCC extraction function with the StandardScaling version\n",
        "for subfolder in os.listdir(source_root_audio):\n",
        "    participant_id, subfolder_num = subfolder.split('_')\n",
        "\n",
        "    # Determine the corresponding CSV file for this subfolder\n",
        "    csv_file = f'{participant_id}_{subfolder_num}_processed.csv'\n",
        "    text_file_path = os.path.join(source_root_text, csv_file)\n",
        "\n",
        "    if not os.path.exists(text_file_path):\n",
        "        print(f'Text file {text_file_path} not found for audio folder {subfolder}. Skipping.')\n",
        "        continue\n",
        "\n",
        "    # Load and aggregate sentences from the CSV\n",
        "    aggregated_sentence = load_and_preprocess_csv(text_file_path)\n",
        "\n",
        "    label_row = labels_df[labels_df['Participant_ID'] == int(participant_id)]\n",
        "\n",
        "    if label_row.empty:\n",
        "        print(f\"Label not found for Participant ID: {participant_id}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    phq_three = label_row['PHQ_Three'].values[0]\n",
        "\n",
        "    subfolder_path = os.path.join(source_root_audio, subfolder)\n",
        "    audio_files = [os.path.join(subfolder_path, f'{participant_id}{subfolder_num}_{i}.wav') for i in range(len(aggregated_sentence.split()))]\n",
        "\n",
        "    # Aggregate MFCC features from the audio files using StandardScaling\n",
        "    aggregated_mfcc = extract_aggregate_mfcc_standard(audio_files)\n",
        "\n",
        "    # Append the aggregated sentence and MFCC to the lists\n",
        "    all_sentences.append(aggregated_sentence)\n",
        "    all_mfcc_features.append(aggregated_mfcc)\n",
        "    label_list.append(phq_three)\n",
        "\n",
        "#%% MFCC Minmaxscaling\n",
        "# Function to extract and aggregate MFCC features from audio files with MinMaxScaling\n",
        "def extract_aggregate_mfcc_minmax(audio_files, max_pad_len=100, n_mfcc=128):\n",
        "    mfcc_features_list = []\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    for audio_path in audio_files:\n",
        "        if os.path.exists(audio_path):\n",
        "            audio, sample_rate = librosa.load(audio_path, sr=None)\n",
        "            mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "\n",
        "            if mfcc.shape[1] < max_pad_len:\n",
        "                pad_width = max_pad_len - mfcc.shape[1]\n",
        "                mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "            else:\n",
        "                mfcc = mfcc[:, :max_pad_len]\n",
        "\n",
        "            # Apply MinMax Scaling\n",
        "            mfcc_scaled = scaler.fit_transform(mfcc.T).T\n",
        "            mfcc_features_list.append(mfcc_scaled.T)\n",
        "\n",
        "    if len(mfcc_features_list) > 0:\n",
        "        # Aggregate MFCC features (e.g., by taking the mean)\n",
        "        aggregated_mfcc = np.mean(mfcc_features_list, axis=0)\n",
        "    else:\n",
        "        aggregated_mfcc = np.zeros((max_pad_len, n_mfcc))  # Fallback if no audio found\n",
        "\n",
        "    return aggregated_mfcc\n",
        "\n",
        "# Replace the MFCC extraction function with the MinMaxScaling version\n",
        "for subfolder in os.listdir(source_root_audio):\n",
        "    participant_id, subfolder_num = subfolder.split('_')\n",
        "\n",
        "    # Determine the corresponding CSV file for this subfolder\n",
        "    csv_file = f'{participant_id}_{subfolder_num}_processed.csv'\n",
        "    text_file_path = os.path.join(source_root_text, csv_file)\n",
        "\n",
        "    if not os.path.exists(text_file_path):\n",
        "        print(f'Text file {text_file_path} not found for audio folder {subfolder}. Skipping.')\n",
        "        continue\n",
        "\n",
        "    # Load and aggregate sentences from the CSV\n",
        "    aggregated_sentence = load_and_preprocess_csv(text_file_path)\n",
        "\n",
        "    label_row = labels_df[labels_df['Participant_ID'] == int(participant_id)]\n",
        "\n",
        "    if label_row.empty:\n",
        "        print(f\"Label not found for Participant ID: {participant_id}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    phq_three = label_row['PHQ_Three'].values[0]\n",
        "\n",
        "    subfolder_path = os.path.join(source_root_audio, subfolder)\n",
        "    audio_files = [os.path.join(subfolder_path, f'{participant_id}{subfolder_num}_{i}.wav') for i in range(len(aggregated_sentence.split()))]\n",
        "\n",
        "    # Aggregate MFCC features from the audio files using MinMaxScaling\n",
        "    aggregated_mfcc = extract_aggregate_mfcc_minmax(audio_files)\n",
        "\n",
        "    # Append the aggregated sentence and MFCC to the lists\n",
        "    all_sentences.append(aggregated_sentence)\n",
        "    all_mfcc_features.append(aggregated_mfcc)\n",
        "    label_list.append(phq_three)\n"
      ],
      "metadata": {
        "id": "bXRCyobVaZMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Tokenize and prepare Word2Vec embedding\n",
        "# Tokenize the sentences\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D, Dropout, LayerNormalization, Multiply\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Load pretrained Word2Vec model\n",
        "word2vec_model = Word2Vec.load(\"D:/EDAIC-WOZ/word2vec_sgns_custom.model\")\n",
        "\n",
        "# Create embedding matrix using the pretrained Word2Vec model\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "vocab_size = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "# Fill the embedding matrix with pretrained word vectors (gensim 4.x compatibility)\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv.key_to_index:  # Use key_to_index in gensim 4.x\n",
        "        embedding_vector = word2vec_model.wv[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define Word2Vec Embedding Layer\n",
        "embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=100,  # max_length\n",
        "                            trainable=True)  # Don't train the embedding weights\n",
        "\n",
        "# Convert the sentences to sequences\n",
        "sequences = tokenizer.texts_to_sequences(all_sentences)\n",
        "\n",
        "# Pad the sequences to ensure uniform length\n",
        "max_length = 100  # Adjust based on your data\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# Convert MFCC features to numpy array\n",
        "mfcc_features = np.array(all_mfcc_features)\n",
        "\n",
        "# Convert labels to categorical format\n",
        "categorical_labels = to_categorical(label_list, num_classes=3)\n",
        "\n",
        "# Define Transformer block\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerBlock, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"rate\": self.rate,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "# Define model using Word2Vec embedding\n",
        "def create_transformer_model(text_input_shape, mfcc_input_shape):\n",
        "    text_inputs = Input(shape=text_input_shape)\n",
        "    embedded_sequences = embedding_layer(text_inputs)\n",
        "\n",
        "    mfcc_inputs = Input(shape=mfcc_input_shape)\n",
        "    mfcc_dense = Dense(128, activation=\"relu\")(mfcc_inputs)\n",
        "\n",
        "    combined = Multiply()([embedded_sequences, mfcc_dense])\n",
        "\n",
        "    transformer_block = TransformerBlock(embed_dim=128, num_heads=4, ff_dim=128)\n",
        "    x = transformer_block(combined)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    outputs = Dense(3, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=[text_inputs, mfcc_inputs], outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# KFold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=119)\n",
        "fold_results = []\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(padded_sequences)):\n",
        "    print(f\"Training fold {fold + 1}/5...\")\n",
        "\n",
        "    X_text_train, X_text_test = padded_sequences[train_index], padded_sequences[test_index]\n",
        "    X_mfcc_train, X_mfcc_test = mfcc_features[train_index], mfcc_features[test_index]\n",
        "    y_train, y_test = categorical_labels[train_index], categorical_labels[test_index]\n",
        "\n",
        "    X_text_train, X_text_val, X_mfcc_train, X_mfcc_val, y_train, y_val = train_test_split(\n",
        "        X_text_train, X_mfcc_train, y_train, test_size=0.2, random_state=119, shuffle=True\n",
        "    )\n",
        "\n",
        "    text_input_shape = (max_length,)\n",
        "    mfcc_input_shape = (100, 128)\n",
        "\n",
        "    model = create_transformer_model(text_input_shape, mfcc_input_shape)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    model_checkpoint = ModelCheckpoint(f'depression_diagnosis_model_fold_{fold+1}.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "    history = model.fit([X_text_train, X_mfcc_train], y_train, epochs=50, batch_size=16,\n",
        "                        validation_data=([X_text_val, X_mfcc_val], y_val),\n",
        "                        callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "    model = tf.keras.models.load_model(f'depression_diagnosis_model_fold_{fold+1}.keras', custom_objects={'TransformerBlock': TransformerBlock})\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    loss, accuracy = model.evaluate([X_text_test, X_mfcc_test], y_test)\n",
        "    print(f'Fold {fold + 1} Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Collect the fold results\n",
        "    fold_results.append((loss, accuracy))\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = model\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict([X_text_test, X_mfcc_test])\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Compute F1 score, precision, recall\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    accuracy = np.mean(y_true_classes == y_pred_classes)\n",
        "\n",
        "    print(f'Fold {fold + 1} F1 Score: {f1:.2f}')\n",
        "    print(f'Fold {fold + 1} Precision: {precision:.2f}')\n",
        "    print(f'Fold {fold + 1} Recall: {recall:.2f}')\n",
        "    print(f'Fold {fold + 1} Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Confusion Matrix for the current fold\n",
        "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "    # Normalize the confusion matrix by row (true classes)\n",
        "    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
        "                xticklabels=['Non-depressed', 'Mildly depressed', 'Severely depressed'],\n",
        "                yticklabels=['Non-depressed', 'Mildly depressed', 'Severely depressed'],\n",
        "                annot_kws={'size': 16})  # Font size for annotations\n",
        "    plt.xlabel('Predicted', fontsize=14)\n",
        "    plt.ylabel('True', fontsize=14)\n",
        "    plt.title(f'Confusion Matrix - Fold {fold + 1}', fontsize=16)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation accuracy and loss values\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'Fold {fold + 1} - Model Accuracy', fontsize=16)\n",
        "    plt.ylabel('Accuracy', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'Fold {fold + 1} - Model Loss', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Calculate and print average loss and accuracy across all folds\n",
        "average_loss = np.mean([result[0] for result in fold_results])\n",
        "average_accuracy = np.mean([result[1] for result in fold_results])\n",
        "print(f\"Average Test Accuracy across 5 folds: {average_accuracy * 100:.2f}%\")\n",
        "print(f\"Average Test Loss across 5 folds: {average_loss:.4f}\")\n",
        "\n",
        "# 모델 저장\n",
        "if best_model is not None:\n",
        "    best_model.save('D:/EDAIC-WOZ/best_model/three_level2/mfcc_minmax_multiply_word2vec.h5')\n",
        "    print(f\"Best model saved as mfcc_minmax_multiply_word2vec.h5\")"
      ],
      "metadata": {
        "id": "wugi5WZ2aeJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec_deltamfcc_Minmax_Multiply"
      ],
      "metadata": {
        "id": "2937N3EIagQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uP6sgf1catm2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
