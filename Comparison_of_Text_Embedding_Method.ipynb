{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SGNS Word2Vec VS Sequential Embedding"
      ],
      "metadata": {
        "id": "ovWF5qItYvnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SGNS Word2Vec\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D, Dropout, LayerNormalization, Multiply\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Load pretrained Word2Vec model\n",
        "word2vec_model = Word2Vec.load(\"D:/EDAIC-WOZ/word2vec_sgns_custom.model\")\n",
        "\n",
        "# Create embedding matrix using the pretrained Word2Vec model\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "vocab_size = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "# Fill the embedding matrix with pretrained word vectors (gensim 4.x compatibility)\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv.key_to_index:  # Use key_to_index in gensim 4.x\n",
        "        embedding_vector = word2vec_model.wv[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define Word2Vec Embedding Layer\n",
        "embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=100,  # max_length\n",
        "                            trainable=True)  # Don't train the embedding weights\n",
        "\n",
        "# Convert the sentences to sequences\n",
        "sequences = tokenizer.texts_to_sequences(all_sentences)\n",
        "\n",
        "# Pad the sequences to ensure uniform length\n",
        "max_length = 100  # Adjust based on your data\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# Convert MFCC features to numpy array\n",
        "mfcc_features = np.array(all_mfcc_features)\n",
        "\n",
        "# Convert labels to categorical format\n",
        "categorical_labels = to_categorical(label_list, num_classes=3)\n",
        "\n",
        "# Define Transformer block\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerBlock, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"rate\": self.rate,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "# Define model using Word2Vec embedding\n",
        "def create_transformer_model(text_input_shape):\n",
        "    text_inputs = Input(shape=text_input_shape)\n",
        "    embedded_sequences = embedding_layer(text_inputs)\n",
        "\n",
        "    transformer_block = TransformerBlock(embed_dim=128, num_heads=4, ff_dim=128)\n",
        "    x = transformer_block(embedded_sequences)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    outputs = Dense(3, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=text_inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# KFold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=119)\n",
        "fold_results = []\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(padded_sequences)):\n",
        "    print(f\"Training fold {fold + 1}/5...\")\n",
        "\n",
        "    X_text_train, X_text_test = padded_sequences[train_index], padded_sequences[test_index]\n",
        "    y_train, y_test = categorical_labels[train_index], categorical_labels[test_index]\n",
        "\n",
        "    X_text_train, X_text_val, y_train, y_val = train_test_split(\n",
        "        X_text_train,  y_train, test_size=0.2, random_state=119, shuffle=True\n",
        "    )\n",
        "\n",
        "    text_input_shape = (max_length,)\n",
        "    mfcc_input_shape = (100, 128)\n",
        "\n",
        "    model = create_transformer_model(text_input_shape)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    model_checkpoint = ModelCheckpoint(f'depression_diagnosis_model_fold_{fold+1}.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "    # 모델 학습\n",
        "    history = model.fit(X_text_train, y_train, epochs=50, batch_size=16,\n",
        "                        validation_data=(X_text_val, y_val),\n",
        "                        callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "    # 모델 평가\n",
        "    loss, accuracy = model.evaluate(X_text_test, y_test)\n",
        "    print(f'Fold {fold + 1} Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # 테스트 데이터 예측\n",
        "    y_pred = model.predict(X_text_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "\n",
        "    # Collect the fold results\n",
        "    fold_results.append((loss, accuracy))\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = model\n",
        "\n",
        "    # Compute F1 score, precision, recall\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "    accuracy = np.mean(y_true_classes == y_pred_classes)\n",
        "\n",
        "    print(f'Fold {fold + 1} F1 Score: {f1:.2f}')\n",
        "    print(f'Fold {fold + 1} Precision: {precision:.2f}')\n",
        "    print(f'Fold {fold + 1} Recall: {recall:.2f}')\n",
        "    print(f'Fold {fold + 1} Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Confusion Matrix for the current fold\n",
        "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "    # Normalize the confusion matrix by row (true classes)\n",
        "    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
        "                xticklabels=['Non-depressed', 'Mildly depressed', 'Severely depressed'],\n",
        "                yticklabels=['Non-depressed', 'Mildly depressed', 'Severely depressed'],\n",
        "                annot_kws={'size': 16})  # Font size for annotations\n",
        "    plt.xlabel('Predicted', fontsize=14)\n",
        "    plt.ylabel('True', fontsize=14)\n",
        "    plt.title(f'Confusion Matrix - Fold {fold + 1}', fontsize=16)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation accuracy and loss values\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'Fold {fold + 1} - Model Accuracy', fontsize=16)\n",
        "    plt.ylabel('Accuracy', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'Fold {fold + 1} - Model Loss', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Calculate and print average loss and accuracy across all folds\n",
        "average_loss = np.mean([result[0] for result in fold_results])\n",
        "average_accuracy = np.mean([result[1] for result in fold_results])\n",
        "print(f\"Average Test Accuracy across 5 folds: {average_accuracy * 100:.2f}%\")\n",
        "print(f\"Average Test Loss across 5 folds: {average_loss:.4f}\")\n",
        "\n",
        "# 모델 저장\n",
        "if best_model is not None:\n",
        "    best_model.save('D:/EDAIC-WOZ/best_model/three_level/only_word2vec.h5')\n",
        "    print(f\"Best model saved as only_word2vec.h5\")\n"
      ],
      "metadata": {
        "id": "QPdDF_8sYRQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Sequential\n",
        "def create_transformer_model(text_input_shape, vocab_size):\n",
        "    text_inputs = Input(shape=text_input_shape)\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=128)(text_inputs)\n",
        "\n",
        "    transformer_block = TransformerBlock(embed_dim=128, num_heads=4, ff_dim=128)\n",
        "    x = transformer_block(embedding_layer)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    outputs = Dense(3, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=text_inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# KFold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(padded_sequences)):\n",
        "    print(f\"Training fold {fold + 1}/5...\")\n",
        "\n",
        "    X_text_train, X_text_test = padded_sequences[train_index], padded_sequences[test_index]\n",
        "    y_train, y_test = categorical_labels[train_index], categorical_labels[test_index]\n",
        "\n",
        "    X_text_train, X_text_val, y_train, y_val = train_test_split(\n",
        "        X_text_train, y_train, test_size=0.2, random_state=42, shuffle=True\n",
        "    )\n",
        "\n",
        "    text_input_shape = (max_length,)\n",
        "    vocab_size = len(word_index) + 1\n",
        "\n",
        "    model = create_transformer_model(text_input_shape, vocab_size)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    model_checkpoint = ModelCheckpoint(f'depression_diagnosis_model_fold_{fold+1}.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "    history = model.fit(X_text_train, y_train, epochs=20, batch_size=32,\n",
        "                        validation_data=(X_text_val, y_val),\n",
        "                        callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "    model = tf.keras.models.load_model(f'depression_diagnosis_model_fold_{fold+1}.keras', custom_objects={'TransformerBlock': TransformerBlock})\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    loss, accuracy = model.evaluate(X_text_test, y_test)\n",
        "    print(f'Fold {fold + 1} Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Collect the fold results\n",
        "    fold_results.append((loss, accuracy))\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_text_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Confusion Matrix for the current fold\n",
        "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Non-depressed', 'Mildly depressed', 'Severely depressed'],\n",
        "                yticklabels=['Non-depressed', 'Mildly depressed', 'Severely depressed'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'Confusion Matrix - Fold {fold + 1}')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation accuracy and loss values\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'Fold {fold + 1} - Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'Fold {fold + 1} - Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Calculate and print average loss and accuracy across all folds\n",
        "average_loss = np.mean([result[0] for result in fold_results])\n",
        "average_accuracy = np.mean([result[1] for result in fold_results])\n",
        "print(f\"Average Test Accuracy across 5 folds: {average_accuracy * 100:.2f}%\")\n",
        "print(f\"Average Test Loss across 5 folds: {average_loss:.4f}\")"
      ],
      "metadata": {
        "id": "febVQ-jiXztm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}